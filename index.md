---
layout: homepage
---

## About Me
**(This site will be deprecated soon)**<br/>
I am a final-year Ph.D. student at UCLA with Prof. [Cho-Jui Hsieh](http://web.cs.ucla.edu/~chohsieh/), and am the founder and current principal of ARC Group - an AIGC Research Collaboration aims at exploring core technologies in multimodal language agent, and eventually productizing the results.
I am also serving as a long-term Student Researcher at Google Research (Ads ML) working on LLM automation, hosted by Prof. [Inderjit S. Dhillon](https://scholar.google.com/citations?user=xBv5ZfkAAAAJ&hl=en), [Felix Yu](https://scholar.google.com/citations?user=lYvF6cUAAAAJ&hl=en) and [Si Si](https://scholar.google.com/citations?user=eAJfUeIAAAAJ&hl=en).

Prior to the era of LLM, I worked on AutoML and Dataset Compression.
I was the recepient of the **[Outstanding Paper Award](https://iclr-conf.medium.com/announcing-iclr-2021-outstanding-paper-awards-9ae0514734ab)** at ICLR 2021.
Besides research, I am also interested in venture capital and entrepreneurial opportunities.

I obtained my B.S. degree (dual) in Computer Science and Statistics from the University of Michigan, with the highest distinction.
During this period, I interned at [Microsoft Research](https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/) and [Sensetime](https://www.sensetime.com/en) on machine learning and computer vision, as well as helped a startup to develop its prototype robots.
Prior to that, I worked on quantitative investing at Shanghai Key Laboratory of Finance.


## Research Overview
My research aims to enable self-improving AI, i.e., to leverage the power of AI Agents to automatize the development & deployment of themselves. To achieve this goal, I founded AIGC Research Collaboration (ARC), a multi-lab research team focused on developing highly automated and trustworthy Multimodal Language Agents.
- LLM-era: core technologies in Multimodal Language Agents, including Safety, Visual Generation, and Prompt Tuning.
- Pre-LLM-era: Efficient ML pipelines, including AutoML and Dataset Compression



## Outreach
- **[ARC Group]** ARC (AIGC Research Collaboration) is a collective of multiple laboratories dedicated to exploring key areas in multimodal foundational agents (LLMs, MLLMs, Diffusion Models, etc.). The team aims at pushing the boundary of SOTA academic research, and eventually commercialize the results. **Since established in Aug 2023, we have expanded to 15 researchers.** For more information on our principles, please refer to [here](https://docs.google.com/presentation/d/1PtRwK6KuqNhExz_ouiu1UDva5Fw6R7PuvrxsVCzSA9U/edit?usp=sharing).

- **[VC/Startups]** I've been interviewing for VC positions. If you are in VC/Startup business and for any reason is looking for people with domain knowledge in A.I., I'd be delighted to have a chat with you.


## Highlighted News
- **[Mar. 2024]** ARC concluded Exploratory Phase with 5 papers. Entering Phase 2, we are doubling down on the winning technologies.
- **[Dec. 2023]** Together with multiple reowned researchers across academia and industry, we are organizing the [1st workshop on Dataset Distillation]() [@CVPR 2024](https://cvpr.thecvf.com/) to explore new frontiers in data-efficient ML. Please stay tuned for more details on submissions, speakers and events!
- **[Oct. 2023]** I will return to Google Research (Ads ML) for another internship hosted by Prof. [Inderjit S. Dhillon](https://scholar.google.com/citations?user=xBv5ZfkAAAAJ&hl=en), [Felix Yu](https://scholar.google.com/citations?user=lYvF6cUAAAAJ&hl=en), and [Si Si](https://scholar.google.com/citations?user=eAJfUeIAAAAJ&hl=en), focusing on advancing the state of Large Language Models.
- **[Aug. 2023]** I formed a Research Alliance to pursue topics in AIGC (see the Outreach section for more info).
- **[Apr. 2023]** [TESLA](https://arxiv.org/abs/2211.10586) is accepted at ICML 2023 - one of the first to scale-up Dataset Distillation to ImageNet-1K.
- **[Jul. 2022]** We released [DC-BENCH](https://justincui03.github.io/dcbench/) - the first benchmark for evaluating Dataset Compression methods.
- **[May. 2022]** I started my internship at Google Research.
- [May 2022] I received Outstanding Graduate Student Award for the Master's degree at UCLA.
- **[Apr. 2021]** Our paper *"Rethinking Architecture Selection in Differentiable NAS"* won the **[Outstanding Paper Award](https://iclr-conf.medium.com/announcing-iclr-2021-outstanding-paper-awards-9ae0514734ab)** at ICLR 2021.


## Experiences (Selected)

- **ARC - AIGC Research Collaboration** - *Founder & Principal* (Aug 2023 - Present)<br/>
Areas: Multimodal Systems<br/>
Advisory Board: [Tianyi Zhou](https://tianyizhou.github.io/), [Minhao Cheng](https://cmhcbb.github.io/), [Cho-Jui Hsieh](http://web.cs.ucla.edu/~chohsieh/)<br/>
Graduate Researchers: [Yuanhao Ban](), [Xirui Li](), [Hengguang Zhou](), [Licheng Lan](), [Andrew Bai](), [Sohyun An](), [Sen Li]()

- **Google Research @AdsML** - *Student Researcher* (Oct 2023 - Present)<br/>
(Multimodal) Large Language Models<br/>
Hosts: [Inderjit S. Dhillon](https://scholar.google.com/citations?user=xBv5ZfkAAAAJ&hl=en), [Felix Yu](https://scholar.google.com/citations?user=lYvF6cUAAAAJ&hl=en), [Si Si](https://scholar.google.com/citations?user=eAJfUeIAAAAJ&hl=en), [Cho-Jui Hsieh](http://web.cs.ucla.edu/~chohsieh/)

- **Google Research** - *Student Researcher* (May 2022 - 2023)<br/>
Text-to-Image Diffusion Models, Efficient Transformers<br/>
Host: Ting Liu and Boqing Gong

- **Microsoft Research Asia** (2019)<br/>
Neural Architecture Search<br/>
Host: [Kai Chen](https://scholar.google.com/citations?hl=en&user=kPDp3cUAAAAJ&view_op=list_works&sortby=pubdate)
  

{% include_relative _includes/publications.md %}

<!-- {% include_relative _includes/services.md %} -->


## Something personal
Currently, my interests are centered around a diverse reading habit and practicing swordsmanship:
- I explore a broad array of topics including history, strategy, science, business, and biographies. Although I once delved deeply into philosophy and psychology, I found these subjects to be increasingly arbitrary and sometimes misleading so I no longer consume these topics.
- For swordsmanship, I study Katori Shintō-Ryū (天真正伝香取神道流). under Masashi Sensei, which is the ealiest martial art systems in Japan that has been passed down unmodified by 23 Shihans（师范）through 600 years. I am instructed by Masashi ShihanDai, with a lineage tracing directly back to Otake Shihan (大竹利典師範).